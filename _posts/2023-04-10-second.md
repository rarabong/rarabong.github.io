---
layout: single
title: "Titanic"
---

# 타이타닉

타이타닉 데이터셋 도전

```
from pathlib import Path
import pandas as pd
import tarfile
import urllib.request

def load_titanic_data():
    tarball_path = Path("datasets/titanic.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/titanic.tgz"
        urllib.request.urlretrieve(url, tarball_path)
        with tarfile.open(tarball_path) as titanic_tarball:
            titanic_tarball.extractall(path="datasets")
    return [pd.read_csv(Path("datasets/titanic") / filename)
            for filename in ("train.csv", "test.csv")]

train_data, test_data = load_titanic_data()
```

데이터는 이미 훈련 세트와 테스트 세트로 분리되어 있습니다. 그러나 테스트 데이터는 레이블을 가지고 있지 않습니다: 훈련 데이터를 이용하여 가능한 최고의 모델을 만들고 테스트 데이터에 대한 예측을 캐글(Kaggle)에 업로드하여 최종 점수를 확인하는 것이 목표입니다.

훈련 세트에서 맨 위 몇 개의 열을 살펴 보겠습니다:


`train_data.head()`

속성은 다음과 같은 의미를 가집니다:

Survived: 타깃입니다. 0은 생존하지 못한 것이고 1은 생존

Pclass: 승객 등급. 1, 2, 3등석.

Name, Sex, Age: 이름 그대로 의미입니다.

SibSp: 함께 탑승한 형제, 배우자의 수.

Parch: 함께 탑승한 자녀, 부모의 수.

Ticket: 티켓 아이디

Fare: 티켓 요금 (파운드)

Cabin: 객실 번호

Embarked: 승객이 탑승한 곳. C(Cherbourg), Q(Queenstown), S(Southampton)

```
train_data = train_data.set_index("PassengerId")
test_data = test_data.set_index("PassengerId")
```
누락된 데이터가 얼마나 되는지 알아보겠습니다:
```
train_data.info()

train_data[train_data["Sex"]=="female"]["Age"].median()
```
괜찮네요. Age, Cabin, Embarked 속성의 일부가 null입니다(891개의 non-null 보다 작습니다). 특히 Cabin은 77%가 null입니다. 일단 Cabin은 무시하고 나머지를 활용하겠습니다. Age는 19%가 null이므로 이를 어떻게 처리할지 결정해야 합니다. null을 중간 나이로 바꾸는 것이 괜찮아 보입니다.

Name과 Ticket 속성도 값을 가지고 있지만 머신러닝 모델이 사용할 수 있는 숫자로 변환하는 것이 조금 까다롭습니다. 그래서 지금은 이 두 속성을 무시하겠습니다.

통계치를 살펴 보겠습니다:

```
train_data.describe()
```
38%만 Survived입니다. :( 거의 40%에 가까우므로 정확도를 사용해 모델을 평가해도 괜찮을 것 같습니다.

평균 Fare는 32.20 파운드라 그렇게 비싸보이지는 않습니다(아마 요금을 많이 반환해 주었기 때문일 것입니다)

평균 Age는 30보다 작습니다.

타깃이 0과 1로 이루어졌는지 확인합니다

```
train_data["Survived"].value_counts()
```
범주형 특성
```
train_data["Pclass"].value_counts()

train_data["Sex"].value_counts()

train_data["Embarked"].value_counts()
```
Embarked 특성은 승객이 탑승한 곳을 알려 줍니다: C=Cherbourg, Q=Queenstown, S=Southampton.

아래 코드는 Pipeline, FeatureUnion와 사용자 정의 DataFrameSelector 클래스를 사용해 각 열을 다르게 전처리합니다. 사이킷런 0.20부터는 이전 장에서처럼 ColumnTransformer를 사용하는 것이 좋습니다.

전처리 파이프라인을 만들어 보죠. 이전 장에서 만든 DataframeSelector를 재사용하여 DataFrame에서 특정 열을 선택하겠습니다:

숫자 특성을 위한 파이프라인을 만듭니다:
```
from sklearn.preprocessing import StandardScaler

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

num_pipeline = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])

```
이제 범주 속성에 대한 파이프라인을 구축할 수 있습니다.
```
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder

cat_pipeline = Pipeline([
        ("ordinal_encoder", OrdinalEncoder()),    
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("cat_encoder", OneHotEncoder(sparse=False)),
    ])
```
이제 범주형 특성을 위한 파이프라인을 만듭니다:
```
from sklearn.compose import ColumnTransformer

num_attribs = ["Age", "SibSp", "Parch", "Fare"]
cat_attribs = ["Pclass", "Sex", "Embarked"]

preprocess_pipeline = ColumnTransformer([
        ("num", num_pipeline, num_attribs),
        ("cat", cat_pipeline, cat_attribs),
    ])
```
이제 우리는 원시 데이터를 가져오고 우리가 원하는 기계 학습 모델에 공급할 수 있는 숫자 입력 기능을 출력하는 멋진 전처리 파이프라인을 가지고 있습니다.
```
X_train = preprocess_pipeline.fit_transform(train_data)
X_train

```
레이블을 얻는 것을 잊지 마십시오
```
y_train = train_data["Survived"]
```
이제 분류기를 훈련할 준비가 되었습니다`RandomForestClassifier`:
```
from sklearn.ensemble import RandomForestClassifier

forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)
forest_clf.fit(X_train, y_train)
```
좋습니다. 우리 모델이 훈련되었으니 이를 사용하여 테스트 세트에 대한 예측을 해봅시다.
```
X_test = preprocess_pipeline.transform(test_data)
y_pred = forest_clf.predict(X_test)
```
그리고 이제 우리는 이러한 예측으로 CSV 파일을 빌드한 다음(Kaggle을 제외한 형식을 존중함) 업로드하고 최선을 다할 수 있습니다.

모델이 얼마나 좋은지 알아보기 위해 교차 검증을 사용하지 않는 이유는 무엇입니까?




```
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)
forest_scores.mean()
```
`SVC`를 사용해 봅시다:
```
from sklearn.svm import SVC

svm_clf = SVC(gamma="auto")
svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)
svm_scores.mean()
```
이 모델이 더 좋아 보입니다

10 폴드 교차 검증에 대한 평균 정확도를 보는 대신 모델에서 얻은 10개의 점수를 1사분위, 3사분위를 명료하게 표현해주는 상자 수염 그림(box-and-whisker) 그래프를 만들어 보겠습니다

```
from matplotlib import pyplot as plt

plt.figure(figsize=(8, 4))
plt.plot([1]*10, svm_scores, ".")
plt.plot([2]*10, forest_scores, ".")
plt.boxplot([svm_scores, forest_scores], labels=("SVM", "Random Forest"))
plt.ylabel("Accuracy")
plt.show()
```
이 결과를 더 향상시키려면:

* 교차 검증과 그리드 탐색을 사용하여 더 많은 모델을 비교하고 하이퍼파라미터를 튜닝하세요.
* 특성 공학을 더 시도해 보세요, 예를 들면:
  * SibSp와 Parch을 이 두 특성의 합으로 바꿉니다.
  * Survived 특성과 관련된 이름을 구별해 보세요(가령, 이름에 "Countess"가 있는 경우 생존할 가능성이 높습니다).
* 수치 특성을 범주형 특성으로 바꾸어 보세요: 예를 들어, 나이대가 다른 경우 다른 생존 비율을 가질 수 있습니다(아래 참조). 그러므로 나이 구간을 범주로 만들어 나이 대신 사용하는 것이 도움이 될 수 있스니다. 비슷하게 생존자의 30%가 혼자 여행하는 사람이기 때문에 이들을 위한 특별한 범주를 만드는 것이 도움이 될 수 있습니다(아래 참조).

```
train_data["AgeBucket"] = train_data["Age"] // 15 * 15
train_data[["AgeBucket", "Survived"]].groupby(['AgeBucket']).mean()
```
```
train_data["RelativesOnboard"] = train_data["SibSp"] + train_data["Parch"]
train_data[["RelativesOnboard", "Survived"]].groupby(
    ['RelativesOnboard']).mean()
```
